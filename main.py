"""
Script principal para ejecutar el scraping y an√°lisis inicial de empleos Big Tech en Jalisco
"""

import os
import sys
import pandas as pd
from datetime import datetime
import logging

# Agregar el directorio src al path
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from src.scraper import AdzunaJobScraper
from src.data_processor import JobDataProcessor, save_processed_data, load_and_process_data

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('jalisco_bigtech_analysis.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


def main():
    """Funci√≥n principal que ejecuta todo el pipeline"""
    print("üöÄ AN√ÅLISIS DE EMPLEOS BIG TECH EN JALISCO")
    print("=" * 50)
    
    try:
        # Paso 1: Scraping de datos
        print("\nüì° PASO 1: Extrayendo datos de empleos...")
        scraper = AdzunaJobScraper()
        raw_df = scraper.scrape_big_tech_jobs_jalisco()
        
        if raw_df.empty:
            print("‚ùå No se pudieron extraer datos. Verifica la configuraci√≥n de la API.")
            return
        
        # Guardar datos raw
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        raw_filename = f"jalisco_bigtech_jobs_raw_{timestamp}.csv"
        raw_filepath = scraper.save_data(raw_df, raw_filename)
        
        print(f"‚úÖ Datos extra√≠dos exitosamente: {len(raw_df)} empleos")
        
        # Paso 2: Procesamiento de datos
        print("\nüîß PASO 2: Procesando y limpiando datos...")
        processor = JobDataProcessor(raw_df)
        
        # Limpiar datos
        cleaned_df = processor.clean_data()
        
        # Crear caracter√≠sticas temporales
        processed_df = processor.create_time_features()
        
        # Preparar para modelado
        model_ready_df, encoders = processor.prepare_for_modeling()
        
        # Guardar datos procesados
        processed_filename = f"jalisco_bigtech_jobs_processed_{timestamp}.csv"
        processed_filepath = save_processed_data(processed_df, processed_filename)
        
        model_ready_filename = f"jalisco_bigtech_jobs_model_ready_{timestamp}.csv"
        model_ready_filepath = save_processed_data(model_ready_df, model_ready_filename)
        
        print(f"‚úÖ Datos procesados exitosamente")
        
        # Paso 3: An√°lisis inicial
        print("\nüìä PASO 3: Generando estad√≠sticas iniciales...")
        stats = processor.get_summary_stats()
        
        # Mostrar resumen detallado
        print("\n" + "="*60)
        print("üìà RESUMEN ESTAD√çSTICO DEL DATASET")
        print("="*60)
        print(f"üìä Total de empleos analizados: {stats['total_jobs']:,}")
        print(f"üè¢ Empresas √∫nicas: {stats['unique_companies']:,}")
        print(f"üåç Ubicaciones √∫nicas: {stats['unique_locations']:,}")
        print(f"üèÜ Empleos Big Tech: {stats['big_tech_jobs']:,} ({stats['big_tech_percentage']:.1f}%)")
        print(f"üí∞ Empleos con informaci√≥n salarial: {stats['jobs_with_salary']:,}")
        
        if stats['avg_salary'] > 0:
            print(f"üíµ Salario promedio: ${stats['avg_salary']:,.0f}")
            print(f"üíµ Salario mediano: ${stats['median_salary']:,.0f}")
        
        if stats['date_range']['start']:
            print(f"üìÖ Rango de fechas: {stats['date_range']['start'].date()} a {stats['date_range']['end'].date()}")
        
        # An√°lisis por empresas
        print(f"\nüîù TOP 10 EMPRESAS CON M√ÅS OFERTAS:")
        if len(processed_df) > 0:
            top_companies = processed_df['company'].value_counts().head(10)
            for i, (company, count) in enumerate(top_companies.items(), 1):
                emoji = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else f"{i:2d}."
                print(f"   {emoji} {company}: {count} ofertas")
        
        # An√°lisis por ubicaciones
        print(f"\nüåç DISTRIBUCI√ìN POR UBICACIONES:")
        if len(processed_df) > 0:
            top_locations = processed_df['location'].value_counts().head(5)
            for location, count in top_locations.items():
                percentage = (count / len(processed_df)) * 100
                print(f"   üìç {location}: {count} empleos ({percentage:.1f}%)")
        
        # An√°lisis de niveles de experiencia
        if 'experience_level' in processed_df.columns:
            print(f"\nüéØ DISTRIBUCI√ìN POR NIVEL DE EXPERIENCIA:")
            exp_levels = processed_df['experience_level'].value_counts()
            for level, count in exp_levels.items():
                percentage = (count / len(processed_df)) * 100
                print(f"   üë®‚Äçüíª {level}: {count} empleos ({percentage:.1f}%)")
        
        # Tecnolog√≠as m√°s demandadas
        print(f"\nüíª TECNOLOG√çAS M√ÅS MENCIONADAS:")
        tech_columns = [col for col in processed_df.columns if col.startswith('mentions_')]
        if tech_columns:
            tech_counts = {}
            for col in tech_columns:
                tech_name = col.replace('mentions_', '').replace('_', ' ').title()
                tech_counts[tech_name] = processed_df[col].sum()
            
            # Ordenar por frecuencia
            sorted_tech = sorted(tech_counts.items(), key=lambda x: x[1], reverse=True)
            for tech, count in sorted_tech[:10]:
                if count > 0:
                    percentage = (count / len(processed_df)) * 100
                    print(f"   ‚ö° {tech}: {count} menciones ({percentage:.1f}%)")
        
        # Series de tiempo b√°sicas
        print(f"\nüìà CREANDO DATOS PARA AN√ÅLISIS TEMPORAL...")
        time_series_daily = processor.create_time_series_data('D')
        time_series_weekly = processor.create_time_series_data('W')
        
        # Guardar series de tiempo
        if not time_series_daily.empty:
            ts_daily_filename = f"jalisco_bigtech_timeseries_daily_{timestamp}.csv"
            save_processed_data(time_series_daily, ts_daily_filename)
            print(f"‚úÖ Series de tiempo diarias guardadas")
        
        if not time_series_weekly.empty:
            ts_weekly_filename = f"jalisco_bigtech_timeseries_weekly_{timestamp}.csv"
            save_processed_data(time_series_weekly, ts_weekly_filename)
            print(f"‚úÖ Series de tiempo semanales guardadas")
        
        # Resumen final
        print("\n" + "="*60)
        print("üéâ PIPELINE DE SCRAPING COMPLETADO EXITOSAMENTE")
        print("="*60)
        print("üìÅ Archivos generados:")
        print(f"   üìÑ Datos raw: {raw_filepath}")
        print(f"   üìÑ Datos procesados: {processed_filepath}")
        print(f"   üìÑ Datos para ML: {model_ready_filepath}")
        if not time_series_daily.empty:
            print(f"   üìÑ Series de tiempo: data/processed/jalisco_bigtech_timeseries_*.csv")
        
        print(f"\nüîÑ Pr√≥ximos pasos recomendados:")
        print(f"   1. üìä Ejecutar an√°lisis exploratorio completo (EDA)")
        print(f"   2. üîç Aplicar t√©cnicas de reducci√≥n de dimensionalidad")
        print(f"   3. üìà Implementar modelos de series de tiempo")
        print(f"   4. ü§ñ Desarrollar modelos predictivos")
        print(f"   5. üìã Generar visualizaciones para el informe")
        
        return processed_df, stats
        
    except Exception as e:
        logger.error(f"Error en el pipeline principal: {e}")
        print(f"‚ùå Error durante la ejecuci√≥n: {e}")
        return None, None


def quick_analysis(filepath: str = None):
    """An√°lisis r√°pido de datos ya extra√≠dos"""
    if filepath is None:
        # Buscar el archivo m√°s reciente
        data_dir = "data/raw"
        if os.path.exists(data_dir):
            files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]
            if files:
                files.sort(reverse=True)
                filepath = os.path.join(data_dir, files[0])
            else:
                print("‚ùå No se encontraron archivos de datos.")
                return
        else:
            print("‚ùå Directorio de datos no encontrado.")
            return
    
    print(f"üìä Analizando datos desde: {filepath}")
    
    try:
        processed_df, stats = load_and_process_data(filepath)
        print("‚úÖ An√°lisis completado")
        return processed_df, stats
    except Exception as e:
        print(f"‚ùå Error durante el an√°lisis: {e}")
        return None, None


if __name__ == "__main__":
    # Verificar argumentos de l√≠nea de comandos
    if len(sys.argv) > 1 and sys.argv[1] == "analyze":
        # Solo an√°lisis de datos existentes
        filepath = sys.argv[2] if len(sys.argv) > 2 else None
        quick_analysis(filepath)
    else:
        # Pipeline completo de scraping y an√°lisis
        main()
