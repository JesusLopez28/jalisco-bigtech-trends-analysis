{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50260c7a",
   "metadata": {},
   "source": [
    "# üöÄ Web Scraping de Empleos Big Tech en Jalisco\n",
    "\n",
    "## An√°lisis de Tendencias de Contrataci√≥n en Empresas de Tecnolog√≠a\n",
    "\n",
    "**Proyecto:** An√°lisis de Datos - 3er Parcial  \n",
    "**Objetivo:** Extraer y analizar datos de ofertas laborales de empresas Big Tech en Jalisco para entender patrones de contrataci√≥n, habilidades demandadas y tendencias salariales.\n",
    "\n",
    "**API Utilizada:** Adzuna Jobs API  \n",
    "**Regi√≥n de Estudio:** Jalisco, M√©xico  \n",
    "**Enfoque:** Empresas como Oracle, Intel, IBM, Microsoft, Google, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### üìã Contenido del Notebook:\n",
    "1. **Configuraci√≥n Inicial** - Librer√≠as y entorno\n",
    "2. **Autenticaci√≥n API** - Credenciales de Adzuna\n",
    "3. **Par√°metros de B√∫squeda** - Definici√≥n de criterios\n",
    "4. **Extracci√≥n de Datos** - Web scraping via API\n",
    "5. **Almacenamiento** - Guardar dataset para an√°lisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148260c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üì¶ CONFIGURACI√ìN INICIAL - IMPORTACI√ìN DE LIBRER√çAS\n",
    "# =============================================================================\n",
    "\n",
    "# Librer√≠as b√°sicas para manejo de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Librer√≠as para web scraping y requests\n",
    "import requests\n",
    "from urllib.parse import urlencode\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Librer√≠as para visualizaci√≥n (opcional para an√°lisis r√°pido)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "# Configuraci√≥n de pandas para mejor visualizaci√≥n\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas exitosamente\")\n",
    "print(f\"üìÖ Timestamp de inicio: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948f8521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üîê AUTENTICACI√ìN CON LA API DE ADZUNA\n",
    "# =============================================================================\n",
    "\n",
    "# Credenciales de la API de Adzuna\n",
    "ADZUNA_APP_ID = \"24b6ac00\"\n",
    "ADZUNA_API_KEY = \"dde84ccd4d8545294d7009fed74ec5ab\"\n",
    "\n",
    "# URLs base de la API\n",
    "ADZUNA_BASE_URL = \"https://api.adzuna.com/v1/api/jobs\"\n",
    "ADZUNA_COUNTRY = \"mx\"  # M√©xico\n",
    "\n",
    "# Headers para las solicitudes HTTP\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept': 'application/json',\n",
    "    'Accept-Language': 'es-MX,es;q=0.9,en;q=0.8',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'DNT': '1',\n",
    "    'Connection': 'keep-alive',\n",
    "}\n",
    "\n",
    "# Configuraci√≥n de rate limiting\n",
    "MAX_REQUESTS_PER_MINUTE = 10\n",
    "DELAY_BETWEEN_REQUESTS = 6  # segundos\n",
    "MAX_RESULTS_PER_PAGE = 50\n",
    "MAX_PAGES_PER_SEARCH = 3\n",
    "\n",
    "print(\"üîê Credenciales configuradas:\")\n",
    "print(f\"   App ID: {ADZUNA_APP_ID}\")\n",
    "print(f\"   API Key: {ADZUNA_API_KEY[:8]}...\")\n",
    "print(f\"   Pa√≠s: {ADZUNA_COUNTRY}\")\n",
    "print(f\"   Rate limit: {MAX_REQUESTS_PER_MINUTE} requests/min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae345b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üéØ DEFINICI√ìN DE PAR√ÅMETROS DE B√öSQUEDA\n",
    "# =============================================================================\n",
    "\n",
    "# Empresas Big Tech a buscar (enfoque en presencia en M√©xico)\n",
    "BIG_TECH_COMPANIES = [\n",
    "    'Oracle', 'Intel', 'IBM', 'Microsoft', 'Google', 'Amazon', 'Apple',\n",
    "    'Meta', 'Facebook', 'Salesforce', 'Adobe', 'SAP', 'Dell', 'HP',\n",
    "    'Cisco', 'VMware', 'NVIDIA', 'Qualcomm', 'Tesla', 'Netflix',\n",
    "    'Uber', 'Airbnb', 'Twitter', 'LinkedIn', 'PayPal', 'eBay',\n",
    "    'Zoom', 'Dropbox', 'Slack', 'Spotify', 'TikTok'\n",
    "]\n",
    "\n",
    "# Palabras clave tecnol√≥gicas relevantes para estudiantes pr√≥ximos a egresar\n",
    "TECH_KEYWORDS = [\n",
    "    'software engineer', 'data scientist', 'machine learning', 'artificial intelligence',\n",
    "    'cloud engineer', 'devops', 'full stack', 'backend', 'frontend', 'mobile developer',\n",
    "    'cybersecurity', 'data analyst', 'product manager', 'scrum master', 'technical lead',\n",
    "    'python developer', 'java developer', 'javascript developer', 'react developer',\n",
    "    'angular developer', 'node.js developer', 'blockchain developer', 'qa engineer',\n",
    "    'ui/ux designer', 'system administrator', 'network engineer', 'database administrator'\n",
    "]\n",
    "\n",
    "# Ubicaciones espec√≠ficas en Jalisco\n",
    "JALISCO_LOCATIONS = [\n",
    "    'Guadalajara', 'Zapopan', 'Tlaquepaque', 'Tonal√°', \n",
    "    'Tlajomulco', 'El Salto', 'Puerto Vallarta', 'Jalisco'\n",
    "]\n",
    "\n",
    "# Niveles de experiencia (importante para estudiantes)\n",
    "EXPERIENCE_LEVELS = [\n",
    "    'junior', 'entry level', 'trainee', 'intern', 'graduate',\n",
    "    'mid level', 'senior', 'lead', 'principal'\n",
    "]\n",
    "\n",
    "print(\"üéØ Par√°metros de b√∫squeda configurados:\")\n",
    "print(f\"   üìä Empresas Big Tech: {len(BIG_TECH_COMPANIES)} empresas\")\n",
    "print(f\"   üíª Keywords t√©cnicos: {len(TECH_KEYWORDS)} t√©rminos\")\n",
    "print(f\"   üåç Ubicaciones Jalisco: {len(JALISCO_LOCATIONS)} ciudades\")\n",
    "print(f\"   üë®‚Äçüíª Niveles de experiencia: {len(EXPERIENCE_LEVELS)} niveles\")\n",
    "\n",
    "# Mostrar algunas empresas como ejemplo\n",
    "print(f\"\\nüè¢ Primeras 10 empresas Big Tech a buscar:\")\n",
    "for i, company in enumerate(BIG_TECH_COMPANIES[:10], 1):\n",
    "    print(f\"   {i:2d}. {company}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ca10fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üï∑Ô∏è FUNCIONES PARA WEB SCRAPING\n",
    "# =============================================================================\n",
    "\n",
    "class AdzunaJobScraper:\n",
    "    \"\"\"Clase para extraer datos de empleos usando la API de Adzuna\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update(HEADERS)\n",
    "        self.request_count = 0\n",
    "        self.start_time = time.time()\n",
    "        self.scraped_jobs = []\n",
    "        \n",
    "    def _rate_limit(self):\n",
    "        \"\"\"Implementa rate limiting para no exceder l√≠mites de la API\"\"\"\n",
    "        self.request_count += 1\n",
    "        elapsed_time = time.time() - self.start_time\n",
    "        \n",
    "        if elapsed_time < 60 and self.request_count >= MAX_REQUESTS_PER_MINUTE:\n",
    "            sleep_time = 60 - elapsed_time + 1\n",
    "            print(f\"‚è≥ Rate limit alcanzado. Esperando {sleep_time:.1f} segundos...\")\n",
    "            time.sleep(sleep_time)\n",
    "            self.request_count = 0\n",
    "            self.start_time = time.time()\n",
    "        \n",
    "        time.sleep(DELAY_BETWEEN_REQUESTS)\n",
    "    \n",
    "    def build_search_url(self, what=\"\", where=\"\", page=1):\n",
    "        \"\"\"Construye la URL de b√∫squeda para la API de Adzuna\"\"\"\n",
    "        params = {\n",
    "            'app_id': ADZUNA_APP_ID,\n",
    "            'app_key': ADZUNA_API_KEY,\n",
    "            'results_per_page': MAX_RESULTS_PER_PAGE,\n",
    "            'what': what,\n",
    "            'where': where,\n",
    "            'content-type': 'application/json',\n",
    "            'page': page\n",
    "        }\n",
    "        \n",
    "        url = f\"{ADZUNA_BASE_URL}/{ADZUNA_COUNTRY}/search/{page}?{urlencode(params)}\"\n",
    "        return url\n",
    "    \n",
    "    def search_jobs(self, what=\"\", where=\"\", max_pages=MAX_PAGES_PER_SEARCH):\n",
    "        \"\"\"Busca empleos usando los par√°metros especificados\"\"\"\n",
    "        all_jobs = []\n",
    "        \n",
    "        print(f\"üîç Buscando: what='{what}', where='{where}'\")\n",
    "        \n",
    "        for page in range(1, max_pages + 1):\n",
    "            try:\n",
    "                self._rate_limit()\n",
    "                \n",
    "                url = self.build_search_url(what=what, where=where, page=page)\n",
    "                \n",
    "                response = self.session.get(url, timeout=30)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                data = response.json()\n",
    "                \n",
    "                if 'results' not in data or not data['results']:\n",
    "                    print(f\"   üìÑ P√°gina {page}: Sin m√°s resultados\")\n",
    "                    break\n",
    "                \n",
    "                jobs = data['results']\n",
    "                all_jobs.extend(jobs)\n",
    "                \n",
    "                print(f\"   üìÑ P√°gina {page}: {len(jobs)} empleos encontrados\")\n",
    "                \n",
    "                if len(jobs) < MAX_RESULTS_PER_PAGE:\n",
    "                    break\n",
    "                    \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"   ‚ùå Error en p√°gina {page}: {e}\")\n",
    "                continue\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"   ‚ùå Error JSON en p√°gina {page}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"   ‚úÖ Total encontrados: {len(all_jobs)} empleos\")\n",
    "        return all_jobs\n",
    "    \n",
    "    def extract_job_details(self, job):\n",
    "        \"\"\"Extrae y limpia los detalles relevantes de un empleo\"\"\"\n",
    "        try:\n",
    "            job_details = {\n",
    "                'id': job.get('id', ''),\n",
    "                'title': job.get('title', ''),\n",
    "                'company': job.get('company', {}).get('display_name', ''),\n",
    "                'location': job.get('location', {}).get('display_name', ''),\n",
    "                'area': ', '.join(job.get('location', {}).get('area', [])),\n",
    "                'salary_min': job.get('salary_min'),\n",
    "                'salary_max': job.get('salary_max'),\n",
    "                'salary_is_predicted': job.get('salary_is_predicted', False),\n",
    "                'description': job.get('description', ''),\n",
    "                'created': job.get('created', ''),\n",
    "                'redirect_url': job.get('redirect_url', ''),\n",
    "                'category': job.get('category', {}).get('label', ''),\n",
    "                'contract_type': job.get('contract_type', ''),\n",
    "                'contract_time': job.get('contract_time', ''),\n",
    "                'latitude': job.get('latitude'),\n",
    "                'longitude': job.get('longitude'),\n",
    "                'scraped_at': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            # Clasificar si es Big Tech\n",
    "            company_name = job_details['company'].lower()\n",
    "            is_big_tech = any(company.lower() in company_name for company in BIG_TECH_COMPANIES)\n",
    "            job_details['is_big_tech'] = is_big_tech\n",
    "            \n",
    "            # Identificar tecnolog√≠as mencionadas\n",
    "            full_text = f\"{job_details['title']} {job_details['description']}\".lower()\n",
    "            mentioned_keywords = [keyword for keyword in TECH_KEYWORDS if keyword.lower() in full_text]\n",
    "            job_details['mentioned_tech_keywords'] = ', '.join(mentioned_keywords)\n",
    "            job_details['tech_keywords_count'] = len(mentioned_keywords)\n",
    "            \n",
    "            return job_details\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error extrayendo detalles: {e}\")\n",
    "            return {}\n",
    "\n",
    "# Inicializar scraper\n",
    "scraper = AdzunaJobScraper()\n",
    "print(\"üï∑Ô∏è Scraper inicializado correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082d57d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üéØ EXTRACCI√ìN DE DATOS - PROCESO PRINCIPAL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üöÄ INICIANDO EXTRACCI√ìN DE DATOS DE EMPLEOS BIG TECH EN JALISCO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_jobs_data = []\n",
    "\n",
    "# Estrategia 1: Buscar por empresas Big Tech espec√≠ficas\n",
    "print(\"\\nüìä ESTRATEGIA 1: B√∫squeda por empresas Big Tech espec√≠ficas\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i, company in enumerate(BIG_TECH_COMPANIES[:8], 1):  # Limitamos a 8 empresas principales\n",
    "    print(f\"\\nüè¢ [{i}/8] Buscando empleos en: {company}\")\n",
    "    \n",
    "    for j, location in enumerate(JALISCO_LOCATIONS[:3], 1):  # Top 3 ubicaciones\n",
    "        print(f\"   üìç Ubicaci√≥n {j}/3: {location}\")\n",
    "        \n",
    "        jobs = scraper.search_jobs(what=company, where=location, max_pages=2)\n",
    "        \n",
    "        for job in jobs:\n",
    "            job_details = scraper.extract_job_details(job)\n",
    "            if job_details and job_details.get('id'):  # Solo agregar si tiene datos v√°lidos\n",
    "                all_jobs_data.append(job_details)\n",
    "\n",
    "print(f\"\\n‚úÖ Estrategia 1 completada: {len(all_jobs_data)} empleos extra√≠dos\")\n",
    "\n",
    "# Estrategia 2: Buscar por keywords t√©cnicos\n",
    "print(\"\\nüíª ESTRATEGIA 2: B√∫squeda por keywords t√©cnicos\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "initial_count = len(all_jobs_data)\n",
    "\n",
    "for i, keyword in enumerate(TECH_KEYWORDS[:10], 1):  # Top 10 keywords m√°s relevantes\n",
    "    print(f\"\\nüîß [{i}/10] Keyword: {keyword}\")\n",
    "    \n",
    "    for j, location in enumerate(JALISCO_LOCATIONS[:2], 1):  # Solo Guadalajara y Zapopan\n",
    "        print(f\"   üìç Ubicaci√≥n {j}/2: {location}\")\n",
    "        \n",
    "        jobs = scraper.search_jobs(what=keyword, where=location, max_pages=2)\n",
    "        \n",
    "        for job in jobs:\n",
    "            job_details = scraper.extract_job_details(job)\n",
    "            if job_details and job_details.get('id'):\n",
    "                all_jobs_data.append(job_details)\n",
    "\n",
    "strategy2_count = len(all_jobs_data) - initial_count\n",
    "print(f\"\\n‚úÖ Estrategia 2 completada: {strategy2_count} nuevos empleos extra√≠dos\")\n",
    "\n",
    "# Estrategia 3: B√∫squeda general de tecnolog√≠a\n",
    "print(\"\\nüåê ESTRATEGIA 3: B√∫squeda general de empleos tech\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "initial_count = len(all_jobs_data)\n",
    "general_terms = ['software', 'technology', 'IT', 'developer', 'engineer']\n",
    "\n",
    "for i, term in enumerate(general_terms, 1):\n",
    "    print(f\"\\nüîç [{i}/5] T√©rmino general: {term}\")\n",
    "    \n",
    "    for j, location in enumerate(JALISCO_LOCATIONS[:2], 1):\n",
    "        print(f\"   üìç Ubicaci√≥n {j}/2: {location}\")\n",
    "        \n",
    "        jobs = scraper.search_jobs(what=term, where=location, max_pages=2)\n",
    "        \n",
    "        for job in jobs:\n",
    "            job_details = scraper.extract_job_details(job)\n",
    "            if job_details and job_details.get('id'):\n",
    "                all_jobs_data.append(job_details)\n",
    "\n",
    "strategy3_count = len(all_jobs_data) - initial_count\n",
    "print(f\"\\n‚úÖ Estrategia 3 completada: {strategy3_count} nuevos empleos extra√≠dos\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"üéâ EXTRACCI√ìN COMPLETADA: {len(all_jobs_data)} empleos totales extra√≠dos\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d117fe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üßπ PROCESAMIENTO Y LIMPIEZA DE DATOS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üßπ Procesando y limpiando datos extra√≠dos...\")\n",
    "\n",
    "# Convertir a DataFrame\n",
    "df_raw = pd.DataFrame(all_jobs_data)\n",
    "\n",
    "print(f\"üìä Datos iniciales: {len(df_raw)} registros, {len(df_raw.columns)} columnas\")\n",
    "\n",
    "if len(df_raw) > 0:\n",
    "    # Eliminar duplicados basados en ID\n",
    "    initial_count = len(df_raw)\n",
    "    df_clean = df_raw.drop_duplicates(subset=['id'], keep='first')\n",
    "    duplicates_removed = initial_count - len(df_clean)\n",
    "    \n",
    "    print(f\"üîÑ Duplicados eliminados: {duplicates_removed}\")\n",
    "    \n",
    "    # Filtrar solo empleos en Jalisco (verificaci√≥n adicional)\n",
    "    jalisco_terms = ['guadalajara', 'zapopan', 'jalisco', 'tlaquepaque', 'tonal√°', 'tlajomulco', 'el salto']\n",
    "    mask_jalisco = df_clean['location'].str.lower().str.contains('|'.join(jalisco_terms), na=False)\n",
    "    df_jalisco = df_clean[mask_jalisco].copy()\n",
    "    \n",
    "    print(f\"üåç Empleos en Jalisco: {len(df_jalisco)} de {len(df_clean)}\")\n",
    "    \n",
    "    # Convertir fechas\n",
    "    if 'created' in df_jalisco.columns:\n",
    "        df_jalisco['created'] = pd.to_datetime(df_jalisco['created'], errors='coerce')\n",
    "    \n",
    "    # Limpiar y convertir salarios\n",
    "    df_jalisco['salary_min'] = pd.to_numeric(df_jalisco['salary_min'], errors='coerce')\n",
    "    df_jalisco['salary_max'] = pd.to_numeric(df_jalisco['salary_max'], errors='coerce')\n",
    "    df_jalisco['salary_avg'] = (df_jalisco['salary_min'] + df_jalisco['salary_max']) / 2\n",
    "    \n",
    "    # Estad√≠sticas b√°sicas\n",
    "    print(\"\\nüìà ESTAD√çSTICAS B√ÅSICAS DEL DATASET:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"   üìä Total empleos √∫nicos: {len(df_jalisco):,}\")\n",
    "    print(f\"   üè¢ Empresas √∫nicas: {df_jalisco['company'].nunique():,}\")\n",
    "    print(f\"   üåç Ubicaciones √∫nicas: {df_jalisco['location'].nunique():,}\")\n",
    "    print(f\"   üèÜ Empleos Big Tech: {df_jalisco['is_big_tech'].sum():,} ({df_jalisco['is_big_tech'].mean()*100:.1f}%)\")\n",
    "    print(f\"   üí∞ Empleos con salario: {df_jalisco['salary_min'].notna().sum():,}\")\n",
    "    \n",
    "    if df_jalisco['salary_avg'].notna().sum() > 0:\n",
    "        avg_salary = df_jalisco['salary_avg'].mean()\n",
    "        median_salary = df_jalisco['salary_avg'].median()\n",
    "        print(f\"   üíµ Salario promedio: ${avg_salary:,.0f}\")\n",
    "        print(f\"   üíµ Salario mediano: ${median_salary:,.0f}\")\n",
    "    \n",
    "    # Top empresas\n",
    "    print(\"\\nüîù TOP 10 EMPRESAS CON M√ÅS OFERTAS:\")\n",
    "    top_companies = df_jalisco['company'].value_counts().head(10)\n",
    "    for i, (company, count) in enumerate(top_companies.items(), 1):\n",
    "        emoji = \"ü•á\" if i == 1 else \"ü•à\" if i == 2 else \"ü•â\" if i == 3 else f\"{i:2d}.\"\n",
    "        print(f\"   {emoji} {company}: {count} ofertas\")\n",
    "    \n",
    "    # Distribuci√≥n por ubicaciones\n",
    "    print(\"\\nüåç DISTRIBUCI√ìN POR UBICACIONES:\")\n",
    "    top_locations = df_jalisco['location'].value_counts().head(5)\n",
    "    for location, count in top_locations.items():\n",
    "        percentage = (count / len(df_jalisco)) * 100\n",
    "        print(f\"   üìç {location}: {count} empleos ({percentage:.1f}%)\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No se encontraron datos para procesar\")\n",
    "    df_jalisco = pd.DataFrame()\n",
    "\n",
    "print(f\"\\n‚úÖ Procesamiento completado. Dataset final: {len(df_jalisco)} empleos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9bc0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üìä AN√ÅLISIS R√ÅPIDO Y VISUALIZACIONES B√ÅSICAS\n",
    "# =============================================================================\n",
    "\n",
    "if len(df_jalisco) > 0:\n",
    "    print(\"üìä Generando an√°lisis y visualizaciones b√°sicas...\")\n",
    "    \n",
    "    # Configurar matplotlib para mejor visualizaci√≥n\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # 1. Distribuci√≥n de empleos por empresa (Top 15)\n",
    "    plt.subplot(2, 3, 1)\n",
    "    top_companies_15 = df_jalisco['company'].value_counts().head(15)\n",
    "    plt.barh(range(len(top_companies_15)), top_companies_15.values)\n",
    "    plt.yticks(range(len(top_companies_15)), top_companies_15.index)\n",
    "    plt.title('üìä Top 15 Empresas por N√∫mero de Ofertas')\n",
    "    plt.xlabel('N√∫mero de Ofertas')\n",
    "    \n",
    "    # 2. Big Tech vs No Big Tech\n",
    "    plt.subplot(2, 3, 2)\n",
    "    big_tech_counts = df_jalisco['is_big_tech'].value_counts()\n",
    "    labels = ['No Big Tech', 'Big Tech']\n",
    "    colors = ['lightcoral', 'lightblue']\n",
    "    plt.pie(big_tech_counts.values, labels=labels, colors=colors, autopct='%1.1f%%')\n",
    "    plt.title('üèÜ Distribuci√≥n Big Tech vs No Big Tech')\n",
    "    \n",
    "    # 3. Distribuci√≥n por ubicaciones\n",
    "    plt.subplot(2, 3, 3)\n",
    "    top_locations_10 = df_jalisco['location'].value_counts().head(10)\n",
    "    plt.bar(range(len(top_locations_10)), top_locations_10.values)\n",
    "    plt.xticks(range(len(top_locations_10)), top_locations_10.index, rotation=45, ha='right')\n",
    "    plt.title('üåç Top 10 Ubicaciones')\n",
    "    plt.ylabel('N√∫mero de Ofertas')\n",
    "    \n",
    "    # 4. Distribuci√≥n de salarios (si hay datos)\n",
    "    plt.subplot(2, 3, 4)\n",
    "    salary_data = df_jalisco['salary_avg'].dropna()\n",
    "    if len(salary_data) > 0:\n",
    "        plt.hist(salary_data, bins=20, alpha=0.7, color='green')\n",
    "        plt.title('üí∞ Distribuci√≥n de Salarios Promedio')\n",
    "        plt.xlabel('Salario (MXN)')\n",
    "        plt.ylabel('Frecuencia')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'Sin datos\\nde salarios', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "        plt.title('üí∞ Distribuci√≥n de Salarios')\n",
    "    \n",
    "    # 5. Empleos por categor√≠a\n",
    "    plt.subplot(2, 3, 5)\n",
    "    if 'category' in df_jalisco.columns and df_jalisco['category'].notna().sum() > 0:\n",
    "        top_categories = df_jalisco['category'].value_counts().head(8)\n",
    "        plt.barh(range(len(top_categories)), top_categories.values)\n",
    "        plt.yticks(range(len(top_categories)), top_categories.index)\n",
    "        plt.title('üìÇ Top Categor√≠as de Empleos')\n",
    "        plt.xlabel('N√∫mero de Ofertas')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'Sin datos\\nde categor√≠as', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "        plt.title('üìÇ Categor√≠as de Empleos')\n",
    "    \n",
    "    # 6. Tecnolog√≠as m√°s mencionadas\n",
    "    plt.subplot(2, 3, 6)\n",
    "    # Contar menciones de tecnolog√≠as\n",
    "    tech_mentions = {}\n",
    "    for keyword in TECH_KEYWORDS[:15]:  # Top 15 keywords\n",
    "        count = df_jalisco['mentioned_tech_keywords'].str.contains(keyword, case=False, na=False).sum()\n",
    "        if count > 0:\n",
    "            tech_mentions[keyword] = count\n",
    "    \n",
    "    if tech_mentions:\n",
    "        tech_sorted = sorted(tech_mentions.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        tech_names, tech_counts = zip(*tech_sorted)\n",
    "        \n",
    "        plt.barh(range(len(tech_names)), tech_counts)\n",
    "        plt.yticks(range(len(tech_names)), tech_names)\n",
    "        plt.title('üíª Top 10 Tecnolog√≠as Mencionadas')\n",
    "        plt.xlabel('N√∫mero de Menciones')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'Sin datos\\nde tecnolog√≠as', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "        plt.title('üíª Tecnolog√≠as Mencionadas')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Mostrar informaci√≥n detallada\n",
    "    print(\"\\nüìã INFORMACI√ìN DETALLADA DEL DATASET:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(df_jalisco.info())\n",
    "    \n",
    "    print(\"\\nüìä PRIMERAS 5 FILAS DEL DATASET:\")\n",
    "    print(\"-\" * 50)\n",
    "    display(df_jalisco.head())\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No hay datos suficientes para generar visualizaciones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79a1b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üíæ ALMACENAMIENTO DEL DATASET\n",
    "# =============================================================================\n",
    "\n",
    "if len(df_jalisco) > 0:\n",
    "    print(\"üíæ Guardando dataset final...\")\n",
    "    \n",
    "    # Crear directorio de datos si no existe\n",
    "    data_dir = \"../data/raw\"\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    # Generar nombre de archivo con timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"jalisco_bigtech_jobs_{timestamp}.csv\"\n",
    "    filepath = os.path.join(data_dir, filename)\n",
    "    \n",
    "    # Guardar dataset\n",
    "    df_jalisco.to_csv(filepath, index=False, encoding='utf-8')\n",
    "    \n",
    "    # Tambi√©n guardar una copia con nombre fijo para f√°cil acceso\n",
    "    latest_filepath = os.path.join(data_dir, \"jalisco_bigtech_jobs_latest.csv\")\n",
    "    df_jalisco.to_csv(latest_filepath, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"‚úÖ Dataset guardado exitosamente:\")\n",
    "    print(f\"   üìÅ Archivo principal: {filepath}\")\n",
    "    print(f\"   üìÅ Archivo latest: {latest_filepath}\")\n",
    "    \n",
    "    # Resumen final del dataset\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üéâ SCRAPING COMPLETADO EXITOSAMENTE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"üìä Dataset final guardado con {len(df_jalisco):,} empleos √∫nicos\")\n",
    "    print(f\"üè¢ {df_jalisco['company'].nunique():,} empresas diferentes\")\n",
    "    print(f\"üåç {df_jalisco['location'].nunique():,} ubicaciones en Jalisco\")\n",
    "    print(f\"üèÜ {df_jalisco['is_big_tech'].sum():,} empleos de empresas Big Tech\")\n",
    "    print(f\"üí∞ {df_jalisco['salary_min'].notna().sum():,} empleos con informaci√≥n salarial\")\n",
    "    \n",
    "    if df_jalisco['created'].notna().sum() > 0:\n",
    "        date_range = f\"{df_jalisco['created'].min().date()} a {df_jalisco['created'].max().date()}\"\n",
    "        print(f\"üìÖ Rango de fechas: {date_range}\")\n",
    "    \n",
    "    print(\"\\nüîÑ Pr√≥ximos pasos recomendados:\")\n",
    "    print(\"   1. üìä An√°lisis exploratorio de datos (EDA) completo\")\n",
    "    print(\"   2. üîç Aplicar t√©cnicas de reducci√≥n de dimensionalidad\")\n",
    "    print(\"   3. üìà An√°lisis de series de tiempo\")\n",
    "    print(\"   4. ü§ñ Modelos predictivos de tendencias\")\n",
    "    print(\"   5. üìã Generar informe final con visualizaciones\")\n",
    "    \n",
    "    # Guardar metadata del scraping\n",
    "    metadata = {\n",
    "        'scraping_date': datetime.now().isoformat(),\n",
    "        'total_jobs': len(df_jalisco),\n",
    "        'unique_companies': df_jalisco['company'].nunique(),\n",
    "        'unique_locations': df_jalisco['location'].nunique(),\n",
    "        'big_tech_jobs': int(df_jalisco['is_big_tech'].sum()),\n",
    "        'jobs_with_salary': int(df_jalisco['salary_min'].notna().sum()),\n",
    "        'api_used': 'Adzuna Jobs API',\n",
    "        'search_strategies': ['company_search', 'keyword_search', 'general_tech_search'],\n",
    "        'target_region': 'Jalisco, M√©xico',\n",
    "        'companies_searched': BIG_TECH_COMPANIES[:8],\n",
    "        'keywords_searched': TECH_KEYWORDS[:10]\n",
    "    }\n",
    "    \n",
    "    metadata_filepath = os.path.join(data_dir, f\"scraping_metadata_{timestamp}.json\")\n",
    "    with open(metadata_filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"   üìã Metadata guardado: {metadata_filepath}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No se pudo guardar el dataset - No hay datos v√°lidos\")\n",
    "    print(\"üîß Verifica:\")\n",
    "    print(\"   - Credenciales de la API\")\n",
    "    print(\"   - Conectividad a internet\")\n",
    "    print(\"   - L√≠mites de rate de la API\")\n",
    "    print(\"   - Par√°metros de b√∫squeda\")\n",
    "\n",
    "print(\"\\nüèÅ Notebook de scraping finalizado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be737f74",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèÅ Conclusiones del Web Scraping\n",
    "\n",
    "### ‚úÖ Logros Alcanzados:\n",
    "- **Extracci√≥n exitosa** de datos de empleos Big Tech en Jalisco usando la API de Adzuna\n",
    "- **Dataset estructurado** con informaci√≥n relevante para an√°lisis de tendencias\n",
    "- **M√∫ltiples estrategias** de b√∫squeda para maximizar la cobertura de datos\n",
    "- **Limpieza y procesamiento** b√°sico de los datos extra√≠dos\n",
    "- **Visualizaciones preliminares** para entender la distribuci√≥n de datos\n",
    "\n",
    "### üìä Datos Obtenidos:\n",
    "- Informaci√≥n de empresas, ubicaciones, salarios y descripciones de empleos\n",
    "- Clasificaci√≥n autom√°tica de empleos Big Tech vs No Big Tech\n",
    "- Extracci√≥n de tecnolog√≠as y habilidades mencionadas\n",
    "- Datos temporales para an√°lisis de series de tiempo\n",
    "\n",
    "### üéØ Siguientes Pasos para el Proyecto:\n",
    "\n",
    "1. **üìà An√°lisis Exploratorio de Datos (EDA)**\n",
    "   - Estad√≠sticas descriptivas detalladas\n",
    "   - Visualizaciones avanzadas con plotly/seaborn\n",
    "   - An√°lisis de correlaciones entre variables\n",
    "\n",
    "2. **üîç Reducci√≥n de Dimensionalidad**\n",
    "   - Aplicar PCA para an√°lisis de componentes principales\n",
    "   - Implementar t-SNE o UMAP para visualizaci√≥n\n",
    "   - Clustering de empleos por caracter√≠sticas similares\n",
    "\n",
    "3. **üìÖ An√°lisis Temporal y Predictivo**\n",
    "   - An√°lisis de estacionalidad en ofertas de empleo\n",
    "   - Modelos ARIMA o Prophet para predicci√≥n de tendencias\n",
    "   - Forecasting de demanda por tecnolog√≠as espec√≠ficas\n",
    "\n",
    "4. **ü§ñ Modelado Avanzado**\n",
    "   - Predicci√≥n de salarios basado en habilidades\n",
    "   - Clasificaci√≥n autom√°tica de nivel de experiencia\n",
    "   - Recomendaci√≥n de habilidades para estudiantes\n",
    "\n",
    "### üí° Insights Preliminares:\n",
    "- Las empresas Big Tech tienen presencia significativa en Jalisco\n",
    "- Guadalajara y Zapopan concentran la mayor√≠a de ofertas\n",
    "- Existe demanda considerable para roles de desarrollo de software\n",
    "- Las tecnolog√≠as modernas (cloud, AI/ML) est√°n en alta demanda\n",
    "\n",
    "---\n",
    "\n",
    "**üìù Nota:** Este dataset servir√° como base para cumplir con todos los requisitos del proyecto de An√°lisis de Datos, incluyendo:\n",
    "- ‚úÖ Fuente de datos reales (API de Adzuna)\n",
    "- ‚úÖ Base para reducci√≥n de dimensionalidad\n",
    "- ‚úÖ Datos temporales para modelado predictivo\n",
    "- ‚úÖ M√∫ltiples variables para visualizaciones avanzadas"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
